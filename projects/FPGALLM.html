<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FPGA LLM - Vishal Moorjani</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <main>
        <h1>FPGA LLM</h1>

        <article class="blog-post">
            <div class="date">April 27, 2025</div>
            <p>I spent some time getting the Vitis suite installed. They don't have the boards used on EC2 F2 instances, so I picked whatever was closest, 
                and since it's been a while since I worked with SystemVerilog I decided a good place to start would probably be with the ALU.
                In ECE385 we learned about different kinds of adders (ripple carry, carry lookahead, carry select) but I'm just going to use the + operation because 
                Vivado will apparently implement the adder that is the most efficient (although if I'm not LUT or DSP contstrained might be interestint to experiment with different kinds).
                I also wrote a testbench for positive and negative numbers and it seems to work. I decided to implement an accumlator instead of an adder, because I can't think
                of a place where you would not be accumulating the results for the forward pass (although as I am writing this I wonder how the position embeddings are added for
                qunatized models, and those might require a regular addition operation). The next step is to implement a multiplier, and then I should have enough to check an int4 matmul.
                I was reading a little bit about how the forward pass happens for quantized models, and it seems like INT8 x INT8 => INT32, then the bias is added, 
                and the output is requantized. This is also something I'll have to implement later. 
                Again, while writing this I wonder if the accumulator is the best way to go about this or not. While performing a matmul, 
                one would multiply a row and a column entry-wise, and then add all of them together. Imagine I perform n multiplications in parallel (based on 
                how many multipliers I have), to add all of them to the same register using the accumulator as it is implemented right now, it would take me n clock cycles 
                (or n - 1 if it just starts out with the value of the first multiplication). Instead, I could create an adder, that 
                adds multiple numbers together, and then that output is what is stored in the register. Perhaps implementing an adder like this would be better, which with the + operation is easy.
                Also, since I will be performing the multiplication first, I would not be adding 4 bit numbers, because the output could be upto 8 bits (-8*-8 = 64).
                It's probably a good idea to implement a 4 bit matmul module first, see how that goes and decide what I need. The simulations are cool to look at anyway.

                <img src="../images/FPGALLM/AccumulatorAddition.png" alt="Accumulator Adder Block Diagram">
                <div class="image-caption">Simulation of accumulator adding [1, 7] => 0x1C (21)</div>

            </p>
        </article>

        <article class="blog-post">
            <div class="date">April 24, 2025</div>
            <p>I first worked on FPGAs + AI in my Digital Systems class where for my final project I ran a model for handwriting recognition on the FPGA we were given. 
                Since MNIST is a relatively easy task, a single linear linear layer performed well enough. I also cut some other corners by converting the inputs into binary, 
                which meant that I didn't need to perform any floating point multiplications. FPGAs don't seem ideal for floating point operations, 
                so it'll be interesting to see if there are any performace gains.
                From what I remember, it took 8 clock cycles for addition and 12-13 for multiplication (I'm not too worried about this because I think I can pipeline to process 12 inputs at a time). 
                I think I'm going to start with a very small model like SmolLM and integer quantization and play it by ear. For the MNIST
                stuff I wrote a Python script to convert the weights (only 1 weight matrix) into a text file with the binary representations of the weights, and loaded that into ROM.
                That worked okay for something with only 7840 parameters, but there's no way that'll work for 135M parameters. It'll also be interesting to 
                figure out tokenization, activation functions, different kinds of attention etc. I'll start by getting Vivado etc. set up and writing a module for INT4 multiplication. 
                From there I can work on matrix muliplications, and then fill in the rest. I should also check what board AWS EC2 F2 instances have, and work off those.</p>
        </article>

    </main>
</body>
</html> 