<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SmolLM2-135M QAT - Vishal Moorjani</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <main>
        <h1>SmolLM2-135M QAT</h1>

        <article class="blog-post">
            <div class="date">July 5, 2025</div>
            <p>
                I've started training my own model. I created custom int only classes for everything. I figured that this would be the easiest to actually implement on the FPGA because I'll be able to 
                walk through each step of the model, tracking things myself. I know that you can do this even with models loaded in with AutoModelForX but I kind of just wanted to do it myself. I'm 
                following a similar strategy to what they do with AQT. I have straight through estimators which allow me to round the weights while still having them be differentiable. Training on the 5M
                sample dataset. I'll see how many tokens it is at some point. Maybe around 50M? I'm currently training with only 2 layers (60M parameters)to make sure that everything is working. 
                I'll step it up to 30 layers (~135M parameters?) as soon as this is done. I'm not expecting anything from the 2 layer model, but as long as the 30 layer one can generate coherent text I'll be happy. 
                I've also restricted the context length to 256 because I don't want to go much further on the FPGA. If this does something cool I'll get into properly training a much bigger model with more data. 
                Love paying for my own compute! Maybe I should use some AWS credits for this. 
            </p>
        </article>

        <article class="blog-post">
            <div class="date">July 4, 2025</div>
            <p>
               Evening: I moved over to RunPod. Colab is fine, but the A100s they have are only 40GB, so with 80GB I can have a bigger batch 
               size. Also, I don't keep losing temporary stuff on RunPod because I created a new nework volume. While creating the dataset, 
               I was thinking why am I writing every example? I could just do it in batches, and that will probably save me a bunch of time. 
               I was right. Writng in batches of 10K samples now and its like 2-3x faster. Once the 5M dataset is done I'll start AQT. 
               I'm using 4 bit weights with 8 bit activations because that is the example they showed on their website. I'll see where to 
               go from there. Also, I was reading the tinygrad docs. They implement 3 kinds of operations, ElementWise, Reduce, and Movement. 
               From this, they are able to implement everything. I could take a similar approach for FPGALLM, but I'll cross that bridge 
               when I come to it. I'm going to start with with the Llama example they have and explore that. The embeddings are not 
               quantized, but I believe they get quantized in the decoder layer. If that's true, I can just skip that and store 
               the quanitzed one. I'm not sure if this is correct though. Let's see how this works. Should I just train my own completely 
               INT model? 
            </p>
            <p>
                Night: Okay so I couldn't find any integer only language models that have no fixed point stuff going on either. I'm going 
                to train one tonight. I want to see first hand why people don't do it. Before jumping into this I looked up some papers on 
                int only training. They use activation functions that are approximations, and they use DFA instead of backprop. Will be 
                interesting to see what's going on. 
            </p>
        </article>

        <article class="blog-post">
            <div class="date">July 2, 2025</div>
            <p>
                Morning: So, it turns out that the dataset never got saved to my drive. I left the generation script running overnight, and though I could see the file 
                on colab, it never got saved to my drive. So, I've just been trying to figure out what went wrong all morning. I've run it a few times after that, 
                and it got up to 100K samples and still got saved. While the script is running I don't see anything in my drive, but I do see it in colab. After interrupting 
                the script, it takes a little bit but it eventually shows up on Drive. After the stuff finished executing last night I disconnected the runtim almost 
                immediately. So, I am not sure if they store files temporarily while they are open on colab, and then persist them to drive only after, while the runtime 
                is still running. I'll stop running once the current run gets to like 1M samples and start working on the INC stuff. I'll run a full 5M run while doing all 
                of that. 
            </p>
            <p>
                Afternoon: Okay I'm pretty sure my hunch was correct, and the reason it didn't get saved to my drive is because I disconnected the runtime immediately after 
                the script was done running, and drive does not constantly persist to drive. It also turns out that the stuff I was looking at for INC was old. INC 2.0+ 
                is much easier to use, no yaml files. Currently tokenizing the 1M dataset. It was taking really long so I restarted it with batching and num_proc = 11. 
                It's going ~8x faster now, which will be especially helpful when I use the full 5M dataset.
            </p>
            <p>
                Later In The Afternoon: So it turns out that INC doesn't support NVIDIA GPUs or TPUs. That means that this is not scalable at all. I do want to see what 
                the output of this whole process looks like though to make sure that I am not msising anything. So, I am going to run INC QAT on the 1M dataset anyway. 
                After that works, I'll move over to torchao. I've been thinking about maybe doing this from scratch. From what I understand, they basically wrap any 
                floating point operation in a quantize + dequantize operation. You then perform the matmul and quantize + dequantize the result. The model learns weights 
                that are more robust to quantization. At inference time, you quantize your weights and perform all the operations in int8. 
            </p>
            <p>
                Evening: I ditched INC. Not having GPU support is insane, so I am now using torchao. This is also quite esay because I'm only running it on 1 GPU, I don't need 
                much more because my model is only 135M parameters. But, I think I would like to set a loftier goal, and I would like to work with Llama 3.1-8B. Anyway, 
                I'll work my way up to that through the SmolLM models. I've started with 135M, then I'll go to their 1.7B model and then Llama 3.1 8B. That should 
                introduce a whole host of issues, so something to look forward to I guess. Anyway, this is running now, I'm waiting for it to finish the 1M dataset 
                before I move onto the 5M one. I've been running this on an L4 while debugging everything. Time to move to an A100.
            </p>
        </article>

        <article class="blog-post">
            <div class="date">July 1, 2025</div>
            <p> Night: It's 3 AM here in champaign, and I've finally made it far enough into FPGALLM to start working on QAT. I remember reading Character AI's post that malhar sent about how
                they serve all their users. They wrote about how they natively train in int8. They still do some dequantization because they aren't compute bound, but their 
                requirements from their models were very differnet from my own, they actually needed their models to be good. I don't really need that for FPGALLM, 
                but hey I get to learn about quantization and let's be honest, it's so much cooler to have real weights. So the reason this is it's own mini-project is 
                because I think it's cool and different enough to deserve it's own project, and FPGALLM doesn't need this at all actually. 
                I've already got a script that will create a dataset for me.That's probably the easist part though. I read Nvidia and Pytorch's guides on QAT. 
                It seems like it should be fairly straightforward but I don't want to make the mistake of getting ahead of myself again. 
                I also haven't worked on colab for a while, so I'm excited to see what they've done with their Gemini integration. 
                The goal is to create a version of SmolLM2-135M that requires no flops. 
            </p>
            <p>
                Later In The Night: It is now almost 4. I was running the data generation script, which is taking quite long because it has to download, clean, and save 
                5 million examples. I forgot to connect to Google Drive before getting half way through, so when I disconnected to connect to a GPU runtime I would have lost 
                all my progress. So I had to restart. Not the best start to this honestly. Anyway, it's saving the dataset to my drive now. Also, I'm not sure if I'm doing something wrong, 
                or Gemini integration is just not finished for Colab yet, because it can't seem to access files in the runtime? Like I asked it to look at the dataset generation script 
                for something, and it said it can't access it. Not sure what's going on there, but hey it's a start. I think I'm going to stop running the dataset generation 
                will before the 5 million samples anyway. I don't want great performance, and I also don't need really long sequences of text because I don't want to have to 
                start off by dealing with such large matmuls anway. It's at 500K right now, I'll let it get to a million and then stop probably. I can always add more later if I need them.
            </p>
            <p>
                Even Later In The Night: I think I've been going about this the wrong way. I'm excited about the FPGALLM stuff, which is why I want to get through this as 
                quickly as possible, and I've been saying I don't need fantastic performance. But, the point of doing this whole thing was to get comofrtable with QAT. So, if anything,
                I should work on getting the best performance out of the quantized model. I was thinking about this when the data gen script was running, so I'm letting it run all the way
                . I will train on 5 million samples. While waiting for it to run, I was talking to ChatGPT about QAT. Turns out, Intel has this thing called neural_compressor, which allows you to 
                perform QAT the model from a single yaml file. It does everything for you, including finding the optimal hyperparameters. 
                I think I'm going to start with INC and see how it goes. Finally I'd want to do something a little more hands on, but let's see how this goes first. 
                4:30 now, and about half way through data gen. Thank god for Google One. 
            </p>
        </article>
    </main>
</body>
</html> 